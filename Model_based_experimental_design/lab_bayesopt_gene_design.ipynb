{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session: model based optimization and experimental design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autumn school, September 2019\n",
    "\n",
    "Written by Javier Gonzalez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab session is to illustrate the concepts seen during the lecture \"Model based optimization and experimental design\". In the first part of the lab we will focus on two aspects of Bayesian Optimization (BO): (1) the choice of the model (2) the choice of the acquisition function. In the second part we will address a simplified version of the gene design case-study detailed in the lecture.\n",
    "\n",
    "The technical material associated to the methods used in this lab can be found in the slides of the leture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will use Emukit (https://amzn.github.io/emukit/). Please be sure that it is correctly installed before starting. The easiest way is using pip. In Ubuntu machines you can do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install emukit\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use GPy (https://sheffieldml.github.io/GPy/). Please be sure that this are installed. With everything installed, you are ready to start. \n",
    "\n",
    "Now, just as in the previous lab, specify to include plots in the notebook and to import relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline  \n",
    "import GPy\n",
    "import emukit\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(12345) # to make sure the lab is repoducible\n",
    "\n",
    "### --- Figure config\n",
    "LEGEND_SIZE = 15\n",
    "FIGURE_SIZE = (8,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Remembering the basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with the lab, remember that (BO) is an heuristic for global optimization of black-box functions. Let $f: {\\mathcal X} \\to R$ be a 'well behaved' continuous function defined on a compact subset ${\\mathcal X} \\subseteq R^d$. Our goal is to solve the global optimization problem of finding\n",
    "$$ x_{M} = \\arg \\min_{x \\in {\\mathcal X}} f(x). $$\n",
    "\n",
    "We assume that $f$ is a *black-box* from which only perturbed evaluations of the type $y_i = f(x_i) + \\epsilon_i$, with $\\epsilon_i \\sim\\mathcal{N}(0,\\sigma^2)$, are  available. The goal is to find $x_M$ by minimizing the number of evaluations of $f$. To do this, we need to determine two crucial bits:\n",
    "\n",
    "1. A **Gaussian process** that will capture the our beliefs on $f$. \n",
    "\n",
    "2. An **acquisition function** that based on the model will be useful to determine where to collect new evaluations of f. \n",
    "\n",
    "Remember that every time a new data point is collected the model is updated and the acquisition function optimized again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running example step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a one-dimensional example. Consider here the Forrester function \n",
    "\n",
    "$$f(x) =(6x-2)^2 \\sin(12x-4),$$ defined on the interval $[0, 1]$. \n",
    "\n",
    "The minimum of this function is located at $x_{min}=0.78$. We assume that the evaluations of $f$ to are perturbed by zero-mean Gaussian noise with standard deviation 0.1. The Forrester function is part of the benchmark of functions of GPyOpt. To create the true function, the perturbed version and the boundaries of the problem you need to run the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from emukit.test_functions import forrester_function\n",
    "from emukit.core.loop.user_function import UserFunctionWrapper\n",
    "from emukit.core import ContinuousParameter, ParameterSpace\n",
    "\n",
    "target_function, space = forrester_function() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The space object defines the input space X=[0,1], which in this case is purely continuous and only one dimensional. In a later section we will see how we can also apply Bayesian optimization in other domains that contain discrete or categorical parameters.\n",
    "\n",
    "Of course in reality, evaluating f on a grid wouldn't be possible, but since the forrester function is a synthetic function we can evaluate it here for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_plot = np.linspace(space.parameters[0].min, space.parameters[0].max, 200)[:, None]\n",
    "y_plot = target_function(x_plot)\n",
    "\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "plt.plot(x_plot, y_plot, \"k\", label=\"Objective Function\")\n",
    "plt.legend(loc=2, prop={'size': LEGEND_SIZE})\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$f(x)$\")\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Intial Design**: Usually, before we start the actual BO loop we need to gather a few observations such that we can fit the model. This is called the initial design and common strategies are either a predefined grid or sampling points uniformly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_init = np.array([[0.2],[0.6], [0.9]])\n",
    "Y_init = target_function(X_init)\n",
    "\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "plt.plot(X_init, Y_init, \"ro\", markersize=10, label=\"Observations\")\n",
    "plt.plot(x_plot, y_plot, \"k\", label=\"Objective Function\")\n",
    "plt.legend(loc=2, prop={'size': LEGEND_SIZE})\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$f(x)$\")\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model**: Now we can start with the BO loop by first fitting a model on the collected data. The arguably most popular model for BO is a Gaussian process (GP) which defines a probability distribution across classes of functions, typically smooth, such that each linear finite-dimensional restriction is multivariate Gaussian (Rasmussen and Williams, 2006). In the next cell we create a model with GPy and we wrap it in Emukit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import GPy\n",
    "from emukit.model_wrappers.gpy_model_wrappers import GPyModelWrapper\n",
    "\n",
    "gpy_model = GPy.models.GPRegression(X_init, Y_init, GPy.kern.RBF(1, lengthscale=0.08, variance=20), noise_var=1e-10)\n",
    "emukit_model = GPyModelWrapper(gpy_model)\n",
    "\n",
    "mu_plot, var_plot = emukit_model.predict(x_plot)\n",
    "\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "plt.plot(X_init, Y_init, \"ro\", markersize=10, label=\"Observations\")\n",
    "plt.plot(x_plot, y_plot, \"k\", label=\"Objective Function\")\n",
    "plt.plot(x_plot, mu_plot, \"C0\", label=\"Model\")\n",
    "plt.fill_between(x_plot[:, 0],\n",
    "                 mu_plot[:, 0] + 2 * np.sqrt(var_plot)[:, 0],\n",
    "                 mu_plot[:, 0] - 2 * np.sqrt(var_plot)[:, 0], color=\"C0\", alpha=0.4)\n",
    "plt.legend(loc=2, prop={'size': LEGEND_SIZE})\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$f(x)$\")\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The acquisition function**: The second thing we need is an acquisition fucntion, the object that we are going to use to design experiments that will get us closer the minimum of $f$. In this tutorial we will use the expected improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from emukit.bayesian_optimization.acquisitions import ExpectedImprovement, NegativeLowerConfidenceBound, ProbabilityOfImprovement\n",
    "\n",
    "ei_acquisition = ExpectedImprovement(emukit_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the current model we can visualize this acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ei_plot = ei_acquisition.evaluate(x_plot)\n",
    "\n",
    "\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "plt.plot(x_plot, (ei_plot - np.min(ei_plot)) / (np.max(ei_plot) - np.min(ei_plot)), \"green\", label=\"EI\")\n",
    "\n",
    "plt.legend(loc=1, prop={'size': LEGEND_SIZE})\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$f(x)$\")\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location where this function reaches its maximum is the place where we need to run the next experiment. We can know which point is that by using a gradient based optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from emukit.core.optimization import GradientAcquisitionOptimizer\n",
    "\n",
    "optimizer = GradientAcquisitionOptimizer(space)\n",
    "x_new, _ = optimizer.optimize(ei_acquisition)\n",
    "print('Location of next experiment: ' + str(x_new[0,0]))\n",
    "\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "plt.plot(x_plot, (ei_plot - np.min(ei_plot)) / (np.max(ei_plot) - np.min(ei_plot)), \"green\", label=\"EI\")\n",
    "plt.axvline(x_new, color=\"red\", label=\"x_next\", linestyle=\"--\")\n",
    "plt.legend(loc=1, prop={'size': LEGEND_SIZE})\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$f(x)$\")\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards we evaluate the true objective function and append it to our initial observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_new = target_function(x_new)  ### run experiment at this location\n",
    "\n",
    "X = np.append(X_init, x_new, axis=0) ### append to the current data set\n",
    "Y = np.append(Y_init, y_new, axis=0)\n",
    "\n",
    "emukit_model.set_data(X, Y) ### update the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_plot, var_plot = emukit_model.predict(x_plot)\n",
    "\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "plt.plot(emukit_model.X, emukit_model.Y, \"ro\", markersize=10, label=\"Observations\")\n",
    "plt.plot(x_plot, y_plot, \"k\", label=\"Objective Function\")\n",
    "plt.plot(x_plot, mu_plot, \"C0\", label=\"Model\")\n",
    "plt.fill_between(x_plot[:, 0],\n",
    "                 mu_plot[:, 0] + 2 * np.sqrt(var_plot)[:, 0],\n",
    "                 mu_plot[:, 0] - 2 * np.sqrt(var_plot)[:, 0], color=\"C0\", alpha=0.4)\n",
    "plt.legend(loc=2, prop={'size': LEGEND_SIZE})\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$f(x)$\")\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running example - automatic loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course in practice we don't want to implement all of these steps our self. Emukit provides a convenient and flexible interface to apply Bayesian optimization. Below we can see how to run Bayesian optimization on the exact same function for 10 iterations.\n",
    "\n",
    "First of all, we need to define the parameter space of the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from emukit.core import ParameterSpace, ContinuousParameter\n",
    "parameter_space = ParameterSpace([ContinuousParameter('x1', 0, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already have defined a model and an acquisition in the previous section, the only thing that we need to do now is to create a 'loop' so we can decide how many experiments we want to run. You can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from emukit.bayesian_optimization.loops import BayesianOptimizationLoop\n",
    "\n",
    "bayesopt_loop = BayesianOptimizationLoop(model = emukit_model,\n",
    "                                         space = parameter_space,\n",
    "                                         acquisition = ei_acquisition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, once the loop is created we run it for some iterations, 10 in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iterations = 10\n",
    "bayesopt_loop.run_loop(target_function, max_iterations)\n",
    "results = bayesopt_loop.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the final result and visualize the best found value after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Optimal location: ' + str(results.minimum_location))\n",
    "print('Value at optimimum :' + str(results.minimum_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_figure = np.array(list(range(len(results.best_found_value_per_iteration))))\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "plt.plot(x_figure,results.best_found_value_per_iteration, \"r-\", markersize=10, label=\"Best observed value\")\n",
    "plt.legend(loc=1, prop={'size': LEGEND_SIZE})\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$f(x)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also visualize the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_plot, var_plot = emukit_model.predict(x_plot)\n",
    "\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "plt.plot(emukit_model.X, emukit_model.Y, \"ro\", markersize=10, label=\"Observations\")\n",
    "plt.plot(x_plot, y_plot, \"k\", label=\"Objective Function\")\n",
    "plt.plot(x_plot, mu_plot, \"C0\", label=\"Model\")\n",
    "plt.fill_between(x_plot[:, 0],\n",
    "                 mu_plot[:, 0] + 2 * np.sqrt(var_plot)[:, 0],\n",
    "                 mu_plot[:, 0] - 2 * np.sqrt(var_plot)[:, 0], color=\"C0\", alpha=0.4)\n",
    "plt.legend(loc=2, prop={'size': LEGEND_SIZE})\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$f(x)$\")\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: optimize your own gene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you work in a lab and you know that the amount of protein produced by a synthetic gene depends on four features that can be extracted from the gene sequence:\n",
    "   - AT ratio (between 0 and 1).\n",
    "   - ATT codon ratio (between 0 and .1).\n",
    "   - CGA codon ratio (between 0 and 0.2).\n",
    "   - Free folding energy (between 0 and 3).\n",
    "\n",
    "Your design space is therefore defined by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from emukit.core import ParameterSpace, ContinuousParameter\n",
    "import numpy as np\n",
    "\n",
    "gene_design_space = ParameterSpace([ContinuousParameter('AT_ratio', 0, 1),\n",
    "                               ContinuousParameter('ATT_ratio', 0, .50),\n",
    "                               ContinuousParameter('CGA_ratio', 0, .50),\n",
    "                               ContinuousParameter('free_energy', 0, 1)]\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each combination of these features you can run a lab experiment that returns the minus amount of protein produced by the gene. Virtually you can run those labs by importing the function test gene from the file weblab as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wetlab import test_gene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, for a gene with AT_ratio = 0.5, ATT_ratio = 0.03, CGA_ratio = 0.05 and free_energy= 2.3 you can extract the minus amount of protein produced by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_gene = np.array([[0.5,0.03,0.05,2.3]])\n",
    "test_gene(new_gene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each experiment costs 1K£ and you have a you have a budget of 60K£. Can you answer the following question \n",
    "\n",
    "**Which is the best possible combination of features in terms of the amount of protein produced by the gene?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
